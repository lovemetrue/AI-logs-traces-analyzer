#!/bin/bash

# Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ð¼ ÐºÐ¾Ñ€Ð½ÐµÐ²ÑƒÑŽ Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸ÑŽ Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð° (Ð¾Ð¿Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾ â€” Ð¼Ð¾Ð¶Ð½Ð¾ Ð·Ð°Ð¿ÑƒÑÐºÐ°Ñ‚ÑŒ Ð² Ð¿ÑƒÑÑ‚Ð¾Ð¹ Ð¿Ð°Ð¿ÐºÐµ)
PROJECT_ROOT="."
cd "$PROJECT_ROOT"

# Ð¤ÑƒÐ½ÐºÑ†Ð¸Ñ Ð´Ð»Ñ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ Ñ„Ð°Ð¹Ð»Ð°, ÐµÑÐ»Ð¸ Ð¾Ð½ Ð½Ðµ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÐµÑ‚
create_file() {
    local filepath="$1"
    mkdir -p "$(dirname "$filepath")"
    if [ ! -f "$filepath" ]; then
        touch "$filepath"
        echo "# Auto-generated file" > "$filepath"
    fi
}

# === 1. ÐŸÐ°Ð¿ÐºÐ° app/ Ð¸ Ð¿Ð¾Ð´Ð¿Ð°Ð¿ÐºÐ¸ ===
create_file "app/__init__.py"
create_file "app/main.py"
create_file "app/config.py"

# models/
create_file "app/models/__init__.py"
create_file "app/models/telemetry.py"
create_file "app/models/training.py"

# services/
create_file "app/services/__init__.py"
create_file "app/services/otel_receiver.py"
create_file "app/services/vector_store.py"
create_file "app/services/ollama_trainer.py"
create_file "app/services/incident_analyzer.py"

# api/
create_file "app/api/__init__.py"
create_file "app/api/routes.py"
create_file "app/api/models.py"

# utils/
create_file "app/utils/__init__.py"
create_file "app/utils/logging.py"
create_file "app/utils/helpers.py"

# === 2. ÐšÐ¾Ñ€Ð½ÐµÐ²Ñ‹Ðµ Ð¿Ð°Ð¿ÐºÐ¸ ===
mkdir -p data/raw
mkdir -p data/processed
mkdir -p models
mkdir -p storage
mkdir -p tests

# === 3. ÐšÐ¾Ñ€Ð½ÐµÐ²Ñ‹Ðµ Ñ„Ð°Ð¹Ð»Ñ‹ ===
create_file "docker-compose.yml"
create_file "Dockerfile"
create_file "requirements.txt"
create_file "pyproject.toml"
create_file ".env.example"
create_file "README.md"

# === 4. Ð”Ð¾Ð±Ð°Ð²Ð¸Ð¼ Ð±Ð°Ð·Ð¾Ð²Ð¾Ðµ ÑÐ¾Ð´ÐµÑ€Ð¶Ð¸Ð¼Ð¾Ðµ Ð² ÐºÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ðµ Ñ„Ð°Ð¹Ð»Ñ‹ (Ð¾Ð¿Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾) ===

# README.md
cat > README.md << 'EOF'
# Observability-to-LLM Pipeline

ÐœÐ¸ÐºÑ€Ð¾ÑÐµÑ€Ð²Ð¸Ñ Ð´Ð»Ñ Ð¿Ñ€Ð¸Ñ‘Ð¼Ð° Ñ‚ÐµÐ»ÐµÐ¼ÐµÑ‚Ñ€Ð¸Ð¸ (Ð»Ð¾Ð³Ð¸, Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸, Ñ‚Ñ€ÐµÐ¹ÑÑ‹) Ð¸Ð· Kubernetes Ð¸ Ð°Ð½Ð°Ð»Ð¸Ð·Ð° Ñ‡ÐµÑ€ÐµÐ· LLM.
EOF

# .env.example
cat > .env.example << 'EOF'
# ÐŸÑ€Ð¸Ð¼ÐµÑ€ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ñ… Ð¾ÐºÑ€ÑƒÐ¶ÐµÐ½Ð¸Ñ
LLM_MODEL=llama3
CHROMA_PATH=./storage/chroma
OLLAMA_HOST=http://localhost:11434
LOG_LEVEL=INFO
EOF

# requirements.txt (Ð¼Ð¸Ð½Ð¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð½Ð°Ð±Ð¾Ñ€)
cat > requirements.txt << 'EOF'
fastapi==0.111.0
uvicorn==0.29.0
pydantic==2.7.1
opentelemetry-api==1.24.0
opentelemetry-sdk==1.24.0
chromadb==0.5.0
sentence-transformers==3.0.1
langchain==0.2.3
ollama==0.1.8
python-dotenv==1.0.1
numpy==1.26.4
pandas==2.2.2
EOF

# pyproject.toml
cat > pyproject.toml << 'EOF'
[build-system]
requires = ["setuptools>=68.0"]
build-backend = "setuptools.build_meta"

[project]
name = "observability-llm"
version = "0.1.0"
description = "LLM-powered observability analyzer"
dependencies = [
    "fastapi",
    "uvicorn",
    "pydantic",
    "chromadb",
    "langchain",
    "ollama",
    "python-dotenv"
]
EOF

# Dockerfile (Ð¿Ñ€Ð¾ÑÑ‚Ð¾Ð¹ ÑˆÐ°Ð±Ð»Ð¾Ð½)
cat > Dockerfile << 'EOF'
FROM python:3.12-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
EOF

# docker-compose.yml
cat > docker-compose.yml << 'EOF'
version: '3.8'

services:
  observability-llm:
    build: .
    ports:
      - "8000:8000"
    environment:
      - CHROMA_PATH=/app/storage/chroma
    volumes:
      - ./storage:/app/storage
    restart: unless-stopped
EOF

echo "âœ… Ð¡Ñ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð° Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð° ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ ÑÐ¾Ð·Ð´Ð°Ð½Ð°!"
echo "ðŸ“ ÐŸÑ€Ð¾Ð²ÐµÑ€ÑŒÑ‚Ðµ ÑÐ¾Ð´ÐµÑ€Ð¶Ð¸Ð¼Ð¾Ðµ Ñ‚ÐµÐºÑƒÑ‰ÐµÐ¹ Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸Ð¸."
